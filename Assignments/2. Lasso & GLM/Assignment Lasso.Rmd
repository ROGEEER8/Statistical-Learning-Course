---
title: 'Assignment 2: Lasso and GLM'
author: "Víctor Villegas, Roger Llorenç, Luis Sierra"
date: "2024-02-27"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading the necessary libraries and preparing the datasets. This time, we will be using a modified version of the Boston dataset, as well as a biological dataset from two files, concerned on survival time dependent on genes.

```{r}
library(glmnet)
load("boston.Rdata")
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)

death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
```

# 1. Lasso for the Boston Housing data

```{r}
# anyNA(boston.c)
boston.c$CHAS <- as.numeric(boston.c$CHAS)
Y <- scale(boston.c$CMEDV, center=TRUE, scale=FALSE)
X <- scale(as.matrix(boston.c[,8:20]), center=TRUE, scale=TRUE) # Exclude response variable

p <- dim(X)[1]
n <- dim(X)[2]
```

After scaling the variables, we use *glmnet* on the Boston dataset to find the Lasso analysis. Note that the cross-validation fit has a probabilistic component, so we declare a seed.

```{r}
set.seed(1234) # To ensure replicability
fit_boston <- glmnet(X, Y)
cv_fit_boston <- cv.glmnet(X, Y)
```

In the following plot we show the path of the coefficients as we decrease the value of the shrinkage parameter $\lambda$.

```{r}
plot(fit_boston)
```
```{r}
plot(cv_fit_boston)
abline(v=log(cv_fit_boston$lambda.min),col=2,lty=2)
abline(v=log(cv_fit_boston$lambda.1se),col=2,lty=2)
```
```{r}

plot(fit_boston, xvar = "lambda")
abline(v = log(cv_fit_boston$lambda.min), col = 2, lty = 2)
abline(v = log(cv_fit_boston$lambda.1se), col = 2, lty = 2)
text(0*(1:p)+(-5),fit_boston$beta[ ,77],colnames(X),pos=4)
```
The minimum $\lambda$ and the coefficients sorted by impact and their values are shown below:

```{r}
cv_fit_boston$lambda.min
coefs_min_boston <- coef(cv_fit_boston, s = "lambda.min")

print("The coefficients sorted by impact for lambda_{min} are: ")
coefs_min_boston_aux = coefs_min_boston[-1,]
coefs_min_boston_aux[order(abs(coefs_min_boston_aux))]
```
From these observations, *"AGE"* (proportion of owner-occupied units built prior to 1940) and *"INDUS"* (proportion of non-retail business acres per town) seem to be the least relevant predictors to determine the median value of a home. 

The most impactful predictors seem to be *"LSTAT"* (% of lower status of the population), *"DIS"* (weighted distances to five Boston employment centers) and *"RM"* (average number of rooms per dwelling).

The 1-SE, or one standard error $\lambda$, and the coefficients sorted by impact and their value are shown below:

```{r}
cv_fit_boston$lambda.1se
coefs_reg_boston <- coef(cv_fit_boston, s = "lambda.1se")

print("The coefficients sorted by impact for lambda_{se} are: ")
coefs_reg_boston_aux = coefs_reg_boston[-1,]
coefs_reg_boston_aux[order(abs(coefs_reg_boston_aux))]
```

From these observations, *"AGE"* and *"INDUS"* are still zero and *"RAD"* (index of accessibility to radial highways) and *"TAX"* ( full-value property-tax rate per $10,000) join the zero-valued coefficients. 

The most impactful predictors are still the same (*"LSTAT"*, *"DIS"* and *"RM"*) with the addition of *"PTRATIO"* (pupil-teacher ratio by town).

# 2. A regression model with $p>>n$:

Using *glmnet* we shall fit, using Lasso with different shrinkage parameters, the logarithm of the survival time against a large set of gene expressions.

### 2.1. In the following two figures we show the optimal value of $\lambda$

```{r}
fit <- glmnet(expr, log.surv)
```

```{r}
plot(fit)
```

Selecting the best shrinkage parameter $\lambda$ by cross validation, again defining a seed to ensure replicability.

```{r}
set.seed(1234) # To ensure replicability
cvfit <- cv.glmnet(expr, log.surv)
```

```{r}
plot(cvfit)
```

```{r}
plot(fit, xvar = "lambda")
abline(v = log(cvfit$lambda.min), col = 2, lty = 2)
abline(v = log(cvfit$lambda.1se), col = 2, lty = 2)
```

Results for $\lambda_{\text{min}}$:
```{r}
cvfit$lambda.min

coefs_min <- coef(cvfit, s = "lambda.min")

print("The coefficients sorted by impact for lambda_{min} are: ")
coefs_min_aux = coefs_min[-1,]; 
sorted_coefs_min = coefs_min_aux[order(abs(coefs_min_aux))]
sorted_coefs_min[sorted_coefs_min != 0]

```
Results for $\lambda_{\text{1se}}$:
```{r}
cvfit$lambda.1se

coefs_reg <- coef(cvfit, s = "lambda.1se")

print("The coefficients sorted by impact for lambda_{min} are: ")
coefs_reg_aux = coefs_reg[-1,]; 
sorted_coefs_reg = coefs_reg_aux[order(abs(coefs_reg_aux))]
sorted_coefs_reg[sorted_coefs_reg != 0]; print("That is, all coefficients are zero.")
```

Only three coefficients are different from zero for the minimum PMSE, and note that for the 1-SE values, the coefficients are all zero and we are left with the null model (always predict the mean average).

### 2.2. Computing fitted values with Lasso parameters.

```{r}
Y_hat <- predict(cvfit, newx = expr, s = "lambda.min")
plot(log.surv, Y_hat, asp=1)
abline(a=0,b=1,col=2)
```

From this plot we can see how, despite there being some correlation, it is indeed rather weak.

### 2.3. OLS with non-zero coefficients set $S_0$ from Lasso fit

```{r}
S_0 <- coefs_min@i # S_0[1] is intercept
ols_model <- lm(log.surv ~ expr[,S_0[2]] + expr[,S_0[3]] + expr[,S_0[4]])
coefs_ols <- ols_model$coefficients
coefs_ols
coefs_min@x
```

Only three coefficients were non-zero after the cross-validation, those corresponding to genes $2252$, $3787$ and $5352$.

What happens now with the predictions? They are, optically, somewhat better to those produced by the Lasso model, and indeed we can see a tendency, albeit still a weak one, between the predictions and the sample data.

```{r}
Y_hat_ols <- predict(ols_model, newx = expr)
plot(log.surv, Y_hat_ols, asp=1)
abline(a=0,b=1,col=2)
```

### 2.4. Comparing fitted Lasso and OLS coefficients:

```{r}
coef_names <- c("Intercept", "G2252", "G3787", "G5352")
names(coefs_ols) <- coef_names
barplot(rbind(coefs_ols, coefs_min@x), xlab='Gene Coefficients', col=c("darkblue","red"), legend = c("OLS", "Lasso"), beside=TRUE)
abline(h=0)
```

We can see how the coefficients for the OLS model are larger than for Lasso, which is to be expected since there is no shrinkage involved. In fact the coefficient for the *5352* gene was almost 0 in the Lasso model (0.0034) but now in the OLS it has a statistically significant amount, with a value of 0.447. In the following chunck we can see a breakdown of the OLS model, where we can see how all predictive variables are statistically significant, but the adjusted $R^2$ is around 0.25, indicating that this model explains a low amount of the variance in the dataset.

```{r}
summary(ols_model)
```

In the following plot we show a comparison between the OLS fitted values and the Lasso fitted values, where we can see a nice tendency. We can also see how the Lasso predicts lower amounts for the points then the OLS, which is completely expected given the shrinkage of the coefficients involved.

```{r}
plot(Y_hat_ols, Y_hat, asp=1)
abline(a=0,b=1,col=2)
```
