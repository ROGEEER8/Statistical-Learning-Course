---
title: 'Assignment 2: Lasso and GLM'
author: "Víctor Villegas, Roger Llorenç, Luis Sierra"
date: "2024-02-27"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading the necessary libraries and preparing the datasets. This time, we will be using a modified version of the Boston dataset, as well as two biological datasets.

```{r}
library(glmnet)
load("boston.Rdata")
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)

death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
```

# 1. Lasso for the Boston Housing data

```{r}
# anyNA(boston.c)
boston.c$CHAS <- as.numeric(boston.c$CHAS)
Y <- scale(boston.c$CMEDV, center=TRUE, scale=FALSE)
X <- scale(as.matrix(boston.c[,8:20]), center=TRUE, scale=TRUE) # Exclude response variable

p <- dim(X)[1]
n <- dim(X)[2]
```

```{r}
fit_boston <- glmnet(X, Y)
```

In the following plot we show the path of the parameters as we decrease the value of the shrinkage parameter $\lambda$.

```{r}
plot(fit_boston)
```

# 2. A regression model with $p>>n$:

Using *glmnet* we shall fit, using LASSO with different shrinkage parameters, the logarithm of the survival time against a large set of gene expressions.

### 2.1. In the following two figures we show the optimal value of $\lambda$

```{r}
fit <- glmnet(expr, log.surv)
```

```{r}
plot(fit)
```

Selecting the best shrinkage parameter $\lambda$ by cross validation

```{r}
cvfit <- cv.glmnet(expr, log.surv)
```

```{r}
plot(cvfit)
```

```{r}
cvfit$lambda.min
cvfit$lambda.1se
coefs_min <- coef(cvfit, s = "lambda.min")
coefs_reg <- coef(cvfit, s = "lambda.1se")
coefs_min@i
coefs_reg@i
```

Only three coefficients are different from zero for the minimum PMSE, and note that for the 1-SE values, the coefficients are all zero and we are left with the null model (always predict the mean average).

### 2.2. Computing fitted values with Lasso parameters.

```{r}
Y_hat <- predict(cvfit, newx = expr, s = "lambda.min")
plot(log.surv, Y_hat, asp=1)
abline(a=0,b=1,col=2)
```

### 2.3. OLS with non-zero coefficients set $S_0$ from Lasso fit

```{r}
S_0 <- coefs_min@i # S_0[1] is intercept
ols_model <- lm(log.surv ~ expr[,S_0[2]] + expr[,S_0[3]] + expr[,S_0[4]])
coefs_ols <- ols_model$coefficients
coefs_ols
coefs_min@x
```

### 2.4. Comparing fitted Lasso and OLS coefficients:

```{r}
coef_names <- c("Intercept", "G2252", "G3787", "G5352")
names(coefs_ols) <- coef_names
barplot(rbind(coefs_ols, coefs_min@x), xlab='Gene Coefficients', col=c("darkblue","red"), legend = c("OLS", "Lasso"), beside=TRUE)
abline(h=0)
```

We can see how the coefficients for the OLS model are larger than for Lasso, which is to be expected since there is no shrinkage involved. In the following chunck we can see a breakdown of the OLS model, where we can see how all predictive variables are statistically significant, but the adjusted $R^2$ is around 0.25, indicating that this model explains a low amount of the variance in the dataset.

```{r}
summary(ols_model)
```

What happens now with the predictions? They are, optically, somewhat better to those produced by the Lasso model, and indeed we can see a tendency, albeit a weak one, between the predictions and the sample data.

```{r}
Y_hat_ols <- predict(ols_model, newx = expr)
plot(log.surv, Y_hat_ols, asp=1)
abline(a=0,b=1,col=2)
```
