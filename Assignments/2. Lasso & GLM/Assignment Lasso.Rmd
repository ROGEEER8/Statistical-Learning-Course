---
title: 'Assignment 2: Lasso and GLM'
author: "Víctor Villegas, Roger Llorenç, Luis Sierra"
date: "2024-02-27"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading the necessary libraries and preparing the datasets. This time, we will be using a modified version of the Boston dataset, as well as two biological datasets.

```{r}
library(glmnet)
load("boston.Rdata")
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)

death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
```

# 1. Lasso for the Boston Housing data

```{r}
# anyNA(boston.c)
boston.c$CHAS <- as.numeric(boston.c$CHAS)
Y <- scale(boston.c$CMEDV, center=TRUE, scale=FALSE)
X <- scale(as.matrix(boston.c[,8:20]), center=TRUE, scale=TRUE) # Exclude response variable

p <- dim(X)[1]
n <- dim(X)[2]
```
glment for boston data

```{r}
fit_boston <- glmnet(X, Y)
cv_fit_boston <- cv.glmnet(X, Y)
```

In the following plot we show the path of the coefficients as we decrease the value of the shrinkage parameter $\lambda$.

```{r}
plot(fit_boston)
```
```{r}
plot(cv_fit_boston)
abline(v=log(cv_fit_boston$lambda.min),col=2,lty=2)
abline(v=log(cv_fit_boston$lambda.1se),col=2,lty=2)
```
```{r}

plot(fit_boston, xvar = "lambda")
abline(v = log(cv_fit_boston$lambda.min), col = 2, lty = 2)
abline(v = log(cv_fit_boston$lambda.1se), col = 2, lty = 2)
text(0*(1:p)+(-5),fit_boston$beta[ ,77],colnames(X),pos=4)
```
The minimum lambda, and the coefficients sorted by impact and their value are shown below:

```{r}
cv_fit_boston$lambda.min
coefs_min_boston <- coef(cv_fit_boston, s = "lambda.min")

print("The coefficients sorted by impact for lambda_{min} are: ")
coefs_min_boston_aux = coefs_min_boston[-1,]
coefs_min_boston_aux[order(abs(coefs_min_boston_aux))]
```
From these observations, "AGE" (proportion of owner-occupied units built prior to 1940) and "INDUS" (proportion of non-retail business acres per town) seem to be the less important predictors to determine the medium value of the home. 

The most important predictors seem to be "LSTAT" (% of lower status of the population), "DIS" (weighted distances to five Boston employment centers) and "RM" (average number of rooms per dwelling).

The one standard error lambda, and the coefficients sorted by impact and their value are shown below:

```{r}
cv_fit_boston$lambda.1se
coefs_reg_boston <- coef(cv_fit_boston, s = "lambda.1se")

print("The coefficients sorted by impact for lambda_{se} are: ")
coefs_reg_boston_aux = coefs_reg_boston[-1,]
coefs_reg_boston_aux[order(abs(coefs_reg_boston_aux))]
```


From these observations, "AGE" and "INDUS" are still zero and "RAD" (index of accessibility to radial highways) and "TAX" ( full-value property-tax rate per $10,000) join the zero-valued coefficients. 

The most important predictors are still the same ("LSTAT", "DIS" and "RM") with the addition of "PTRATIO" (pupil-teacher ratio by town).

# 2. A regression model with $p>>n$:

Using *glmnet* we shall fit, using LASSO with different shrinkage parameters, the logarithm of the survival time against a large set of gene expressions.

### 2.1. In the following two figures we show the optimal value of $\lambda$

```{r}
fit <- glmnet(expr, log.surv)
```

```{r}
plot(fit)
```

Selecting the best shrinkage parameter $\lambda$ by cross validation

```{r}
cvfit <- cv.glmnet(expr, log.surv)
```

```{r}
plot(cvfit)
```

```{r}
cvfit$lambda.min
cvfit$lambda.1se
coefs_min <- coef(cvfit, s = "lambda.min")
coefs_reg <- coef(cvfit, s = "lambda.1se")
coefs_min@i
coefs_reg@i
```

Only three coefficients are different from zero for the minimum PMSE, and note that for the 1-SE values, the coefficients are all zero and we are left with the null model (always predict the mean average).

### 2.2. Computing fitted values with Lasso parameters.

```{r}
Y_hat <- predict(cvfit, newx = expr, s = "lambda.min")
plot(log.surv, Y_hat, asp=1)
abline(a=0,b=1,col=2)
```

### 2.3. OLS with non-zero coefficients set $S_0$ from Lasso fit

```{r}
S_0 <- coefs_min@i # S_0[1] is intercept
ols_model <- lm(log.surv ~ expr[,S_0[2]] + expr[,S_0[3]] + expr[,S_0[4]])
coefs_ols <- ols_model$coefficients
coefs_ols
coefs_min@x
```

### 2.4. Comparing fitted Lasso and OLS coefficients:

```{r}
coef_names <- c("Intercept", "G2252", "G3787", "G5352")
names(coefs_ols) <- coef_names
barplot(rbind(coefs_ols, coefs_min@x), xlab='Gene Coefficients', col=c("darkblue","red"), legend = c("OLS", "Lasso"), beside=TRUE)
abline(h=0)
```

We can see how the coefficients for the OLS model are larger than for Lasso, which is to be expected since there is no shrinkage involved. In the following chunck we can see a breakdown of the OLS model, where we can see how all predictive variables are statistically significant, but the adjusted $R^2$ is around 0.25, indicating that this model explains a low amount of the variance in the dataset.

```{r}
summary(ols_model)
```

What happens now with the predictions? They are, optically, somewhat better to those produced by the Lasso model, and indeed we can see a tendency, albeit a weak one, between the predictions and the sample data.

```{r}
Y_hat_ols <- predict(ols_model, newx = expr)
plot(log.surv, Y_hat_ols, asp=1)
abline(a=0,b=1,col=2)
```
