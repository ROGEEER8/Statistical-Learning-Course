---
title: "3. ROC Curve"
author: "Víctor Villegas, Roger Llorenç, Luis Sierra"
date: "2024-03-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Read the SPAM dataset

```{r}
spam <- read.table("spambase/spambase.data",sep=",")

spam.names <- c(read.table("spambase/spambase.names",sep=":",skip=33,nrows=53,as.is=TRUE)[,1],
                "char_freq_#",
                read.table("spambase/spambase.names",sep=":",skip=87,nrows=3,as.is=TRUE)[,1],
                "spam.01")

names(spam) <- spam.names 

n<-dim(spam)[1]
p<-dim(spam)[2]-1

spam.01 <- spam[,p+1]
spam.vars <- as.matrix(spam[,1:p])

cat(paste("n = ", n, ', p = ', p,sep=""))
cat(paste("Proportion of spam e-mails =", round(mean(spam.01), 2), sep=""))

glm.spam <- glm(spam.01 ~ spam.vars,family=binomial)
# summary(glm.spam)

```

### 2. Train-Test partioning of the data

The manual way to create a train-test split is shown below.

Note the composition of the data, which can be divided as follows:

4601 initial data split into train (2/3) -> 3068 and test (1/3) -> 1533
spam_train (2/3) -> 1209 and nospam_train (2/3) -> 1859
spam_test (1/3) -> 604 and nospam_test (1/3) -> 929


```{r}
set.seed(1234)

spamIndex <- which(spam.01 %in% c(1))
nospamIndex <- which(spam.01 %in% c(0))

spamTrain <- sample(spamIndex, size = ceiling(2/3*length(spamIndex)))
nospamTrain <- sample(nospamIndex, size = ceiling(2/3*length(nospamIndex)))
trainIndex <- union(nospamTrain, spamTrain)

trainData <- spam[trainIndex,]
testData <- spam[-trainIndex,]

n <- dim(trainData)[1]
p <- dim(trainData)[2] - 1

Y <- scale(trainData[,p+1], center=FALSE, scale=FALSE)
X <- scale(trainData[,1:p], center=TRUE, scale=TRUE)
```



```{r, include=FALSE}
# using the caret package
library(caret)

set.seed(1234)

trainIndex <- createDataPartition(
  spam.01,
  times=1,
  p=2/3,
  list=FALSE
)
cat(paste("Proportion of training spam e-mails =",round(mean(spam.01[trainIndex]),2),sep=""))
cat(paste("Proportion of training spam e-mails =",round(mean(spam.01[-trainIndex]),2),sep=""))
```

### 3. Classification rules

We now consider three methods to obtain classification rules: logistic regression via maximum likelihood, logistic regression via Lasso, and $k$-nearest neighbors.

Starting with logistic regression via maximum likelihood:

```{r}
glm.logistic <- glm(spam.01 ~ ., data = as.data.frame(trainData), family = binomial())

predictions_glm <- predict(glm.logistic, newdata = testData[,1:p], type = "response")

#summary(glm.logistic)
```

Lasso GLM:

```{r}
library(glmnet)
set.seed(1234)

fit_lasso <- glmnet(X, Y, family = "binomial")
cvfit_lasso = cv.glmnet(X, Y, family = "binomial", type.measure = "class")

plot(fit_lasso, xvar = "dev", label = TRUE)
plot(cvfit_lasso)

```

```{r}
final_fit_lasso <- glmnet(trainData[,1:p], trainData$spam.01, family = "binomial", lambda = cvfit_lasso$lambda.min)
newx <- model.matrix(testData$spam.01 ~ ., data=testData[,1:p])
predictions_lasso <- predict(final_fit_lasso, newx = newx[,-1], type = "response")
```

$k$-nearest neighbours:

```{r}
set.seed(1234)

trainData$spam.01 <- as.factor(trainData$spam.01)
ctrl <- trainControl(method="repeatedcv",repeats = 3)
knn_fit <- train(spam.01 ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:20),
             trControl  = ctrl,
             data       = trainData)
knn_fit$bestTune # 
```

The best performing value for $k$ was $k=1$.

```{r}
knn_fit
plot(knn_fit, print.thres = 0.5, type="S")
```

```{r}
predictions_knn <- predict(knn_fit, newdata = testData)
testData$spam.01 <- as.factor(testData$spam.01)
confusionMatrix(predictions_knn, testData$spam.01)
```


### 4. ROC Curves

Using the test data we find the following ROC curves:

```{r}
library(pROC)

# glm ROC
roc_curve_glm <- roc(testData[,p+1], predictions_glm)
plot(roc_curve_glm, main = "ROC Curve", col = "blue", title="Classic GLM")

# Lasso ROC
roc_curve_lasso <- roc(testData[,p+1], predictions_lasso)
plot(roc_curve_lasso, main = "ROC Curve", col = "blue", title="Lasso GLM")

# knn ROC
roc_curve_knn <- roc(testData[,p+1], as.numeric(as.character(predictions_knn)))
plot(roc_curve_knn, main = "ROC Curve", col = "blue", title="Knn fit")
```

### 5. Misclassification Rate at $c=\frac{1}{2}$

```{r}
predicted_classes_glm <- ifelse(predictions_glm >= 0.5, 1, 0)
misclassification_rate_glm <- mean(predicted_classes_glm != testData$spam.01)

cat(paste("missclassification rate for GLM using c = 1/2:",misclassification_rate_glm,sep=" "))


predicted_classes_lasso <- ifelse(predictions_lasso >= 0.5, 1, 0)
misclassification_rate_lasso <- mean(predicted_classes_lasso != testData$spam.01)
cat(paste("missclassification rate for Lasso using c = 1/2:",misclassification_rate_lasso,sep=" "))

predicted_classes_knn <- ifelse(as.numeric(as.character(predictions_knn)) >= 0.5, 1, 0)
misclassification_rate_knn <- mean(predicted_classes_knn != testData$spam.01)
cat(paste("missclassification rate for 1-nn using c = 1/2:",misclassification_rate_knn,sep=" "))
```

### 6. Calculating $\ell_{val}$

```{r}
l_val_glm_store <- numeric(dim(testData)[1])

for (j in 1:dim(testData)[1]){
  aux1 = as.numeric(testData$spam.01[j]) * log(predictions_glm[j])
  aux2 = (1-as.numeric(testData$spam.01[j])) * log(1-predictions_glm[j])
  l_val_glm_store[j] = aux1 + aux2
  if (is.nan(aux1) | is.nan(aux2) | l_val_glm_store[j] == -Inf ) {
    l_val_glm_store[j] = 0
  }
  
}
l_val_glm <- mean(l_val_glm_store)

###

l_val_lasso_store = numeric(dim(testData)[1])

for (j in 1:dim(testData)[1]){
  aux1 = as.numeric(testData$spam.01[j]) * log(predictions_lasso[j])
  aux2 = (1-as.numeric(testData$spam.01[j])) * log(1- predictions_lasso[j])
  l_val_lasso_store[j] = aux1 + aux2
  if (is.nan(aux1) | is.nan(aux2) | l_val_lasso_store[j] == -Inf ) {
    l_val_lasso_store[j] = 0
  }
  
}

l_val_lasso <- mean(l_val_lasso_store)

###

preds <- as.numeric(as.character(predictions_knn))

l_val_knn_store <- numeric(dim(testData)[1])

for (j in 1:dim(testData)[1]){
  aux1 = as.numeric(testData$spam.01[j]) * log(preds[j])
  aux2 = (1-as.numeric(testData$spam.01[j])) * log(1-preds[j])
  l_val_knn_store[j] = aux1 + aux2
  if (is.nan(aux1) | is.nan(aux2) | l_val_knn_store[j] == -Inf ) {
    l_val_knn_store[j] = 0
  }
  
}

l_val_knn <- mean(l_val_knn_store)
```

```{r}
cat(paste("l_val for classical GLM",l_val_glm,sep=" "))
cat(paste("l_val for Lasso GLM",l_val_lasso,sep=" "))
cat(paste("l_val for 1-nn",l_val_knn,sep=" "))
```

